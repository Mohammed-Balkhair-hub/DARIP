"""
Headliner Node - Condenses articles into crisp headings and key facts

Processes articles from queried_news.json and uses OpenAI to extract
verifiable facts, dates, concrete nouns, and numbers into bullet-point headlines.
"""

import json
import os
from datetime import datetime, timezone
from typing import Any, Dict, List

from openai import OpenAI
from tenacity import retry, stop_after_attempt, wait_exponential

from config import settings


@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
def _call_openai_for_headline(full_text: str, settings_obj: Any) -> str:
    """
    Call OpenAI API to generate headline facts from article full text.
    
    Args:
        full_text: The full text content of the article
        settings_obj: Settings object containing OpenAI configuration
        
    Returns:
        String containing bullet-point headlines generated by the LLM
        
    Raises:
        Exception: If OpenAI API call fails after retries
    """
    client = OpenAI(api_key=settings_obj.OPENAI_API_KEY)
    
    system_prompt = (
        "You are a precise news condenser. Extract verifiable facts from the provided full_text. "
        "Quote ≤ 25 consecutive words per quote. Never speculate. "
        "Prefer dates, concrete nouns, and numbers. "
        "Output exactly the Headliner as bullet points starting with •."
    )
    
    try:
        response = client.chat.completions.create(
            model=settings_obj.OPENAI_MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": full_text}
            ],
            temperature=0.2,
            max_tokens=settings_obj.OPENAI_MAX_TOKENS
        )
        
        return response.choices[0].message.content.strip()
    
    except Exception as e:
        print(f"[headliner] OpenAI API error: {e}")
        raise


def run_headliner(state: Dict[str, Any]) -> Dict[str, Any]:
    """
    LangGraph node function: Process articles and generate headlines.
    
    This node:
    1. Extracts items and output_dir from state
    2. Processes each article with non-empty full_text
    3. Calls OpenAI to generate headline bullet points
    4. Saves results to headliners.json
    5. Returns updated state with headliners_file path
    
    Args:
        state: Dictionary containing:
            - items: List of article dictionaries
            - output_dir: Path to output directory
            
    Returns:
        Updated state dictionary with headliners_file added
    """
    print("[headliner] ===== STARTING HEADLINER NODE =====")
    
    items = state.get("items", [])
    output_dir = state.get("output_dir", "")
    
    print(f"[headliner] Processing {len(items)} articles...")
    
    processed_items = []
    skipped_count = 0
    
    for idx, item in enumerate(items):
        item_id = item.get("item_id", "unknown")
        title = item.get("title", "")
        source = item.get("source", "")
        url = item.get("url", "")
        full_text = item.get("full_text", "")
        
        # Skip articles with empty full_text
        if not full_text or not full_text.strip():
            skipped_count += 1
            continue
        
        print(f"[headliner] Processing {idx + 1}/{len(items)}: {title[:60]}...")
        
        try:
            # Call OpenAI to generate headlines
            headlines = _call_openai_for_headline(full_text, settings)
            
            processed_items.append({
                "item_id": item_id,
                "title": title,
                "source": source,
                "url": url,
                "headlines": headlines
            })
            
        except Exception as e:
            print(f"[headliner] Error processing item {item_id}: {e}")
            skipped_count += 1
            continue
    
    # Save results to JSON
    output_file = os.path.join(output_dir, "headliners.json")
    
    output_data = {
        "generated_at": datetime.now(timezone.utc).isoformat(),
        "input_file": state.get("input_file", ""),
        "total_processed": len(processed_items),
        "total_skipped": skipped_count,
        "items": processed_items
    }
    
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(output_data, f, indent=2, ensure_ascii=False)
    
    print(f"[headliner] Processed {len(processed_items)} articles")
    print(f"[headliner] Skipped {skipped_count} articles (empty full_text)")
    print(f"[headliner] Saved to: {output_file}")
    print("[headliner] ===== HEADLINER NODE COMPLETE =====")
    
    # Update state with headliners file path
    state["headliners_file"] = output_file
    
    return state

